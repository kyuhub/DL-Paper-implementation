{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aedee8",
   "metadata": {},
   "source": [
    "## Load Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "375b2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f89e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.9.1  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e41d9",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2849095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA(data.Dataset):\n",
    "    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n",
    "        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.attr_path = attr_path\n",
    "        self.selected_attrs = selected_attrs\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.train_dataset = []\n",
    "        self.test_dataset = []\n",
    "        self.attr2idx = {}\n",
    "        self.idx2attr = {}\n",
    "        self.preprocess()\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.num_images = len(self.train_dataset)\n",
    "        else:\n",
    "            self.num_images = len(self.test_dataset)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n",
    "        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n",
    "        all_attr_names = lines[1].split()\n",
    "        for i, attr_name in enumerate(all_attr_names):\n",
    "            self.attr2idx[attr_name] = i\n",
    "            self.idx2attr[i] = attr_name\n",
    "\n",
    "        lines = lines[2:]\n",
    "        random.seed(1234)\n",
    "        random.shuffle(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            split = line.split()\n",
    "            filename = split[0]\n",
    "            values = split[1:]\n",
    "\n",
    "            label = []\n",
    "            for attr_name in self.selected_attrs:\n",
    "                idx = self.attr2idx[attr_name]\n",
    "                label.append(values[idx] == '1')\n",
    "\n",
    "            if (i+1) < 2000:\n",
    "                self.test_dataset.append([filename, label])\n",
    "            else:\n",
    "                self.train_dataset.append([filename, label])\n",
    "\n",
    "        print('Finished preprocessing the CelebA dataset...')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
    "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
    "        filename, label = dataset[index]\n",
    "        image = Image.open(os.path.join(self.image_dir, filename))\n",
    "        return self.transform(image), torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images.\"\"\"\n",
    "        return self.num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bbea91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n",
    "               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    transform = []\n",
    "    if mode == 'train':\n",
    "        transform.append(T.RandomHorizontalFlip())\n",
    "        # train mode일 경우 Data Augumentation을 위해 RamdomHorizontalFip\n",
    "    transform.append(T.CenterCrop(crop_size))\n",
    "    transform.append(T.Resize(image_size))\n",
    "    transform.append(T.ToTensor())\n",
    "    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
    "    transform = T.Compose(transform)\n",
    "    # 각 이미지마다 CenterCrop, Resize, Normalize\n",
    "    # 대부분의 이미지 DataLoader에 쓰이는 일반적인 조합\n",
    "\n",
    "    if dataset == 'CelebA':\n",
    "        dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n",
    "        # CelebA는 구조가 복잡하므로, 앞에서 별도로 Class를 선언하여 전처리함\n",
    "        \n",
    "    elif dataset == 'RaFD':\n",
    "        dataset = ImageFolder(image_dir, transform)\n",
    "        # RaFD는 특별히 복잡한 구성을 가진 데이터셋이 아니므로,\n",
    "        # PyTorch 내장 ImageFolder 함수로도 충분\n",
    "\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=(mode=='train'),\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd7492",
   "metadata": {},
   "source": [
    "- 하나의 network를 이용하려면 input의 크기가 동일해야함\n",
    "- 비율과 크기를 맞추기 위해 center 부분을 crop하고 resize 진행\n",
    "- center 부분을 crop 함으로서 얼굴 부분을 더 강조할 수 있음\n",
    "\n",
    "\n",
    "- CelebA: 178*218 -> 178*178 -> 128*128 40 labels\n",
    "- RaFD: 256*256 -> 178*178 -> 128*128 8 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81510b0e",
   "metadata": {},
   "source": [
    "## ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5e01e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
    "            # Conv2d + InstaceNorm2d 2개 사이에 ReLU Activation Fuction이 끼어있는 구조\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "        # ResidualBlock: input과 output을 더해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d270a",
   "metadata": {},
   "source": [
    "-  Batch Normalization이 아닌 Instance Normalization을 사용하고 있어서 찾아봄\n",
    "- 둘의 공식은 같지만 Batch Normalization은 전체 Dataset 기준으로 Batch를 Normalize 함\n",
    "- Instance Normalization은 Batch 단위로 Instance들을 Normalize 함\n",
    "- stackoverflow.com/questions/45463778/instance-normalisation-vs-batch-normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f93e6d",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1332d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-sampling layers.\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        # Bottleneck layers.\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
    "\n",
    "        # Up-sampling layers.\n",
    "        for i in range(2):\n",
    "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Replicate spatially and concatenate domain information.\n",
    "        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n",
    "        # This is because instance normalization ignores the shifting (or bias) effect.\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.main(x)\n",
    "        \n",
    "        # condition이 이미지 사이즈에 맞춰 확대되고 conatenate되는 방식으로 학습이 이루어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0cd57d",
   "metadata": {},
   "source": [
    "- 대칭적인 조건으로 Downsampling과 Upsampling이 이루어지는 중간에 ResidualBlock을 Bottleneck Layer로 사용\n",
    "- Generator인 만큼 입력값과 같은 사이즈의 출력값을 내놓도록 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e683346",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36bf1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.01))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_src = self.conv1(h)\n",
    "        out_cls = self.conv2(h)\n",
    "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68084fa6",
   "metadata": {},
   "source": [
    "- Generator, ResidualBlock과 달리 ReLU가 아닌 LeakyReLU를 사용\n",
    "- 이미지가 아닌 클래스 자체에 대한 분포도 학습하므로,\n",
    "    conv1은 이미지를 학습, conv2는 클래스를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a4412",
   "metadata": {},
   "source": [
    "## Bulid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bc0b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"Tensorboard logger.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Initialize summary writer.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Add scalar summary.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f5a23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    \"\"\"Solver for training and testing StarGAN.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize configurations.\"\"\"\n",
    "\n",
    "        # Data loader.\n",
    "        self.celeba_loader = celeba_loader\n",
    "        self.rafd_loader = rafd_loader\n",
    "\n",
    "        # Model configurations.\n",
    "        self.c_dim = config.c_dim\n",
    "        self.c2_dim = config.c2_dim\n",
    "        self.image_size = config.image_size\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.g_repeat_num = config.g_repeat_num\n",
    "        self.d_repeat_num = config.d_repeat_num\n",
    "        self.lambda_cls = config.lambda_cls\n",
    "        self.lambda_rec = config.lambda_rec\n",
    "        self.lambda_gp = config.lambda_gp\n",
    "\n",
    "        # Training configurations.\n",
    "        self.dataset = config.dataset\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_iters = config.num_iters\n",
    "        self.num_iters_decay = config.num_iters_decay\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.n_critic = config.n_critic\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.resume_iters = config.resume_iters\n",
    "        self.selected_attrs = config.selected_attrs\n",
    "\n",
    "        # Test configurations.\n",
    "        self.test_iters = config.test_iters\n",
    "\n",
    "        # Miscellaneous.\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Directories.\n",
    "        self.log_dir = config.log_dir\n",
    "        self.sample_dir = config.sample_dir\n",
    "        self.model_save_dir = config.model_save_dir\n",
    "        self.result_dir = config.result_dir\n",
    "\n",
    "        # Step size.\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.model_save_step = config.model_save_step\n",
    "        self.lr_update_step = config.lr_update_step\n",
    "\n",
    "        # Build the model and tensorboard.\n",
    "        self.build_model()\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Create a generator and a discriminator.\"\"\"\n",
    "        if self.dataset in ['CelebA', 'RaFD']:\n",
    "            self.G = Generator(self.g_conv_dim, self.c_dim, self.g_repeat_num)\n",
    "            self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim, self.d_repeat_num) \n",
    "        elif self.dataset in ['Both']:\n",
    "            self.G = Generator(self.g_conv_dim, self.c_dim+self.c2_dim+2, self.g_repeat_num)   # 2 for mask vector.\n",
    "            self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim+self.c2_dim, self.d_repeat_num)\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
    "        self.print_network(self.G, 'G')\n",
    "        self.print_network(self.D, 'D')\n",
    "            \n",
    "        self.G.to(self.device)\n",
    "        self.D.to(self.device)\n",
    "\n",
    "    def print_network(self, model, name):\n",
    "        \"\"\"Print out the network information.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(model)\n",
    "        print(name)\n",
    "        print(\"The number of parameters: {}\".format(num_params))\n",
    "\n",
    "    def restore_model(self, resume_iters):\n",
    "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
    "        print('Loading the trained models from step {}...'.format(resume_iters))\n",
    "        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
    "        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
    "        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
    "        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    def build_tensorboard(self):\n",
    "        \"\"\"Build a tensorboard logger.\"\"\"\n",
    "        from logger import Logger\n",
    "        self.logger = Logger(self.log_dir)\n",
    "\n",
    "    def update_lr(self, g_lr, d_lr):\n",
    "        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n",
    "        for param_group in self.g_optimizer.param_groups:\n",
    "            param_group['lr'] = g_lr\n",
    "        for param_group in self.d_optimizer.param_groups:\n",
    "            param_group['lr'] = d_lr\n",
    "\n",
    "    def reset_grad(self):\n",
    "        \"\"\"Reset the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def denorm(self, x):\n",
    "        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp_(0, 1)\n",
    "\n",
    "    def gradient_penalty(self, y, x):\n",
    "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "        weight = torch.ones(y.size()).to(self.device)\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "        return torch.mean((dydx_l2norm-1)**2)\n",
    "\n",
    "    def label2onehot(self, labels, dim):\n",
    "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        out = torch.zeros(batch_size, dim)\n",
    "        out[np.arange(batch_size), labels.long()] = 1\n",
    "        return out\n",
    "\n",
    "    def create_labels(self, c_org, c_dim=5, dataset='CelebA', selected_attrs=None):\n",
    "        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n",
    "        # Get hair color indices.\n",
    "        if dataset == 'CelebA':\n",
    "            hair_color_indices = []\n",
    "            for i, attr_name in enumerate(selected_attrs):\n",
    "                if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n",
    "                    hair_color_indices.append(i)\n",
    "\n",
    "        c_trg_list = []\n",
    "        for i in range(c_dim):\n",
    "            if dataset == 'CelebA':\n",
    "                c_trg = c_org.clone()\n",
    "                if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n",
    "                    c_trg[:, i] = 1\n",
    "                    for j in hair_color_indices:\n",
    "                        if j != i:\n",
    "                            c_trg[:, j] = 0\n",
    "                else:\n",
    "                    c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
    "            elif dataset == 'RaFD':\n",
    "                c_trg = self.label2onehot(torch.ones(c_org.size(0))*i, c_dim)\n",
    "\n",
    "            c_trg_list.append(c_trg.to(self.device))\n",
    "        return c_trg_list\n",
    "\n",
    "    def classification_loss(self, logit, target, dataset='CelebA'):\n",
    "        \"\"\"Compute binary or softmax cross entropy loss.\"\"\"\n",
    "        if dataset == 'CelebA':\n",
    "            return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)\n",
    "        elif dataset == 'RaFD':\n",
    "            return F.cross_entropy(logit, target)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train StarGAN within a single dataset.\"\"\"\n",
    "        # Set data loader.\n",
    "        if self.dataset == 'CelebA':\n",
    "            data_loader = self.celeba_loader\n",
    "        elif self.dataset == 'RaFD':\n",
    "            data_loader = self.rafd_loader\n",
    "\n",
    "        # Fetch fixed inputs for debugging.\n",
    "        data_iter = iter(data_loader)\n",
    "        x_fixed, c_org = next(data_iter)\n",
    "        x_fixed = x_fixed.to(self.device)\n",
    "        c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n",
    "\n",
    "        # Learning rate cache for decaying.\n",
    "        g_lr = self.g_lr\n",
    "        d_lr = self.d_lr\n",
    "\n",
    "        # Start training from scratch or resume training.\n",
    "        start_iters = 0\n",
    "        if self.resume_iters:\n",
    "            start_iters = self.resume_iters\n",
    "            self.restore_model(self.resume_iters)\n",
    "\n",
    "        # Start training.\n",
    "        print('Start training...')\n",
    "        start_time = time.time()\n",
    "        for i in range(start_iters, self.num_iters):\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                             1. Preprocess input data                                #\n",
    "            # =================================================================================== #\n",
    "            \n",
    "            '''\n",
    "            generator의 input으로 target domain과 input image를 channel-wise concatination을 통해 만든 input을 사용한다.\n",
    "            channel-wise로 concatination하기 위해서는 domain의 height, width를 image의 height, width와 맞춰야 하기 때문에\n",
    "            각 label과 mask vector의 size를 128*128로 확장한다.\n",
    "            생성된 target domain을 input image와 chnnel-wise로 concatination해서 input으로 사용한다.\n",
    "            '''\n",
    "            \n",
    "            # Fetch real images and labels.\n",
    "            try:\n",
    "                x_real, label_org = next(data_iter)\n",
    "            except:\n",
    "                data_iter = iter(data_loader)\n",
    "                x_real, label_org = next(data_iter)\n",
    "\n",
    "            # Generate target domain labels randomly.\n",
    "            rand_idx = torch.randperm(label_org.size(0))\n",
    "            label_trg = label_org[rand_idx]\n",
    "\n",
    "            if self.dataset == 'CelebA':\n",
    "                c_org = label_org.clone()\n",
    "                c_trg = label_trg.clone()\n",
    "            elif self.dataset == 'RaFD':\n",
    "                c_org = self.label2onehot(label_org, self.c_dim)\n",
    "                c_trg = self.label2onehot(label_trg, self.c_dim)\n",
    "\n",
    "            x_real = x_real.to(self.device)           # Input images.\n",
    "            c_org = c_org.to(self.device)             # Original domain labels.\n",
    "            c_trg = c_trg.to(self.device)             # Target domain labels.\n",
    "            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n",
    "            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                             2. Train the discriminator                              #\n",
    "            # =================================================================================== #\n",
    "\n",
    "            # Compute loss with real images.\n",
    "            '''\n",
    "            real/fake에 대한 예측과, domain에 대한 예측을 이용해 loss를 계산\n",
    "            '''\n",
    "            out_src, out_cls = self.D(x_real)\n",
    "            d_loss_real = - torch.mean(out_src)\n",
    "            d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset)\n",
    "\n",
    "            # Compute loss with fake images.\n",
    "            '''\n",
    "            real/fake 부분만을 이용해서 loss를 계산\n",
    "            '''\n",
    "            x_fake = self.G(x_real, c_trg)\n",
    "            out_src, out_cls = self.D(x_fake.detach())\n",
    "            d_loss_fake = torch.mean(out_src)\n",
    "\n",
    "            # Compute loss for gradient penalty.\n",
    "            '''\n",
    "            wasserstein gan loss를 사용하기 위해 존재하는데, \n",
    "            이 부분을 real과 fake data 사이의 어떤 부분에서도 gradient norm이 1에 가깝게 하는 것을 목표\n",
    "            '''\n",
    "            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n",
    "            x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n",
    "            out_src, _ = self.D(x_hat)\n",
    "            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n",
    "\n",
    "            # Backward and optimize.\n",
    "            '''\n",
    "            구해진 loss를 더하고 backward and optimize를 진행\n",
    "            '''\n",
    "            d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp\n",
    "            self.reset_grad()\n",
    "            d_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "\n",
    "            # Logging.\n",
    "            loss = {}\n",
    "            loss['D/loss_real'] = d_loss_real.item()\n",
    "            loss['D/loss_fake'] = d_loss_fake.item()\n",
    "            loss['D/loss_cls'] = d_loss_cls.item()\n",
    "            loss['D/loss_gp'] = d_loss_gp.item()\n",
    "            \n",
    "            # =================================================================================== #\n",
    "            #                               3. Train the generator                                #\n",
    "            # =================================================================================== #\n",
    "            \n",
    "            if (i+1) % self.n_critic == 0:\n",
    "                \n",
    "                # Original-to-target domain.\n",
    "                '''\n",
    "                생성된 fake image를 discriminator의 forward 과정을 통해 얻은 값을 이용해 loss를 계산하는 부분이다. \n",
    "                real/fake인지에 대한 fake_loss, domain인 정확하게 분류했는지에 대한 cls_loss\n",
    "                '''\n",
    "                x_fake = self.G(x_real, c_trg)\n",
    "                out_src, out_cls = self.D(x_fake)\n",
    "                g_loss_fake = - torch.mean(out_src)\n",
    "                g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset)\n",
    "\n",
    "                # Target-to-original domain.\n",
    "                '''\n",
    "                reconstruction loss를 구하는 부분이다.\n",
    "                이 부분은 fake image를 기존의 input image로 재생성하는 부분이다. \n",
    "                input image와 reconstructed image가 최대한 같게 만들어주는 것이 목표다. \n",
    "                이 loss를 사용하는 것은 content는 보존하고, style만 바뀌길 원해서이다. \n",
    "                fake image를 이용해서 input image를 다시 재생성하지 못한다면, \n",
    "                이는 content가 소실된 것을 의미하고, content를 구별할 수 없을 정도로 변했을 수도 있다.\n",
    "                '''\n",
    "                x_reconst = self.G(x_fake, c_org)\n",
    "                g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n",
    "\n",
    "                # Backward and optimize.\n",
    "                '''\n",
    "                계산한 모든 로스를 더하는 부분이다. 이때 lambda라는 변수를 이용해 가중치를 부여할 수 있다. \n",
    "                예를 들어 reconstrion_loss에 lambda 값으로 10을 곱했다면, \n",
    "                network가 reconstruction 부분에 대해 더 좋은 결과를 얻으려고 해, \n",
    "                변환 결과인 fake image에서 content 정보가 보다 잘 보존돼있을 것이다. \n",
    "                물론 한 부분을 강화함으로써 다른 부분에서 취약해질 수 있다.\n",
    "                '''\n",
    "                g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls\n",
    "                self.reset_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                # Logging.\n",
    "                loss['G/loss_fake'] = g_loss_fake.item()\n",
    "                loss['G/loss_rec'] = g_loss_rec.item()\n",
    "                loss['G/loss_cls'] = g_loss_cls.item()\n",
    "\n",
    "            # =================================================================================== #\n",
    "            #                                 4. Miscellaneous                                    #\n",
    "            # =================================================================================== #\n",
    "\n",
    "            # Print out training information.\n",
    "            if (i+1) % self.log_step == 0:\n",
    "                et = time.time() - start_time\n",
    "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
    "                for tag, value in loss.items():\n",
    "                    log += \", {}: {:.4f}\".format(tag, value)\n",
    "                print(log)\n",
    "\n",
    "                if self.use_tensorboard:\n",
    "                    for tag, value in loss.items():\n",
    "                        self.logger.scalar_summary(tag, value, i+1)\n",
    "\n",
    "            # Translate fixed images for debugging.\n",
    "            if (i+1) % self.sample_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    x_fake_list = [x_fixed]\n",
    "                    for c_fixed in c_fixed_list:\n",
    "                        x_fake_list.append(self.G(x_fixed, c_fixed))\n",
    "                    x_concat = torch.cat(x_fake_list, dim=3)\n",
    "                    sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n",
    "                    save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
    "                    print('Saved real and fake images into {}...'.format(sample_path))\n",
    "\n",
    "            # Save model checkpoints.\n",
    "            if (i+1) % self.model_save_step == 0:\n",
    "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
    "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
    "                torch.save(self.G.state_dict(), G_path)\n",
    "                torch.save(self.D.state_dict(), D_path)\n",
    "                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
    "\n",
    "            # Decay learning rates.\n",
    "            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
    "                g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
    "                d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
    "                self.update_lr(g_lr, d_lr)\n",
    "                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
    "\n",
    "    def train_multi(self):\n",
    "        \"\"\"Train StarGAN with multiple datasets.\"\"\"        \n",
    "        # Data iterators.\n",
    "        celeba_iter = iter(self.celeba_loader)\n",
    "        rafd_iter = iter(self.rafd_loader)\n",
    "\n",
    "        # Fetch fixed inputs for debugging.\n",
    "        x_fixed, c_org = next(celeba_iter)\n",
    "        x_fixed = x_fixed.to(self.device)\n",
    "        c_celeba_list = self.create_labels(c_org, self.c_dim, 'CelebA', self.selected_attrs)\n",
    "        c_rafd_list = self.create_labels(c_org, self.c2_dim, 'RaFD')\n",
    "        zero_celeba = torch.zeros(x_fixed.size(0), self.c_dim).to(self.device)           # Zero vector for CelebA.\n",
    "        zero_rafd = torch.zeros(x_fixed.size(0), self.c2_dim).to(self.device)             # Zero vector for RaFD.\n",
    "        mask_celeba = self.label2onehot(torch.zeros(x_fixed.size(0)), 2).to(self.device)  # Mask vector: [1, 0].\n",
    "        mask_rafd = self.label2onehot(torch.ones(x_fixed.size(0)), 2).to(self.device)     # Mask vector: [0, 1].\n",
    "\n",
    "        # Learning rate cache for decaying.\n",
    "        g_lr = self.g_lr\n",
    "        d_lr = self.d_lr\n",
    "\n",
    "        # Start training from scratch or resume training.\n",
    "        start_iters = 0\n",
    "        if self.resume_iters:\n",
    "            start_iters = self.resume_iters\n",
    "            self.restore_model(self.resume_iters)\n",
    "\n",
    "        # Start training.\n",
    "        print('Start training...')\n",
    "        start_time = time.time()\n",
    "        for i in range(start_iters, self.num_iters):\n",
    "            for dataset in ['CelebA', 'RaFD']:\n",
    "\n",
    "                # =================================================================================== #\n",
    "                #                             1. Preprocess input data                                #\n",
    "                # =================================================================================== #\n",
    "                \n",
    "                # Fetch real images and labels.\n",
    "                data_iter = celeba_iter if dataset == 'CelebA' else rafd_iter\n",
    "                \n",
    "                try:\n",
    "                    x_real, label_org = next(data_iter)\n",
    "                except:\n",
    "                    if dataset == 'CelebA':\n",
    "                        celeba_iter = iter(self.celeba_loader)\n",
    "                        x_real, label_org = next(celeba_iter)\n",
    "                    elif dataset == 'RaFD':\n",
    "                        rafd_iter = iter(self.rafd_loader)\n",
    "                        x_real, label_org = next(rafd_iter)\n",
    "\n",
    "                # Generate target domain labels randomly.\n",
    "                rand_idx = torch.randperm(label_org.size(0))\n",
    "                label_trg = label_org[rand_idx]\n",
    "\n",
    "                if dataset == 'CelebA':\n",
    "                    c_org = label_org.clone()\n",
    "                    c_trg = label_trg.clone()\n",
    "                    zero = torch.zeros(x_real.size(0), self.c2_dim)\n",
    "                    mask = self.label2onehot(torch.zeros(x_real.size(0)), 2)\n",
    "                    c_org = torch.cat([c_org, zero, mask], dim=1)\n",
    "                    c_trg = torch.cat([c_trg, zero, mask], dim=1)\n",
    "                elif dataset == 'RaFD':\n",
    "                    c_org = self.label2onehot(label_org, self.c2_dim)\n",
    "                    c_trg = self.label2onehot(label_trg, self.c2_dim)\n",
    "                    zero = torch.zeros(x_real.size(0), self.c_dim)\n",
    "                    mask = self.label2onehot(torch.ones(x_real.size(0)), 2)\n",
    "                    c_org = torch.cat([zero, c_org, mask], dim=1)\n",
    "                    c_trg = torch.cat([zero, c_trg, mask], dim=1)\n",
    "\n",
    "                x_real = x_real.to(self.device)             # Input images.\n",
    "                c_org = c_org.to(self.device)               # Original domain labels.\n",
    "                c_trg = c_trg.to(self.device)               # Target domain labels.\n",
    "                label_org = label_org.to(self.device)       # Labels for computing classification loss.\n",
    "                label_trg = label_trg.to(self.device)       # Labels for computing classification loss.\n",
    "\n",
    "                # =================================================================================== #\n",
    "                #                             2. Train the discriminator                              #\n",
    "                # =================================================================================== #\n",
    "\n",
    "                # Compute loss with real images.\n",
    "                out_src, out_cls = self.D(x_real)\n",
    "                out_cls = out_cls[:, :self.c_dim] if dataset == 'CelebA' else out_cls[:, self.c_dim:]\n",
    "                d_loss_real = - torch.mean(out_src)\n",
    "                d_loss_cls = self.classification_loss(out_cls, label_org, dataset)\n",
    "\n",
    "                # Compute loss with fake images.\n",
    "                x_fake = self.G(x_real, c_trg)\n",
    "                out_src, _ = self.D(x_fake.detach())\n",
    "                d_loss_fake = torch.mean(out_src)\n",
    "\n",
    "                # Compute loss for gradient penalty.\n",
    "                alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n",
    "                x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n",
    "                out_src, _ = self.D(x_hat)\n",
    "                d_loss_gp = self.gradient_penalty(out_src, x_hat)\n",
    "\n",
    "                # Backward and optimize.\n",
    "                d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp\n",
    "                self.reset_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # Logging.\n",
    "                loss = {}\n",
    "                loss['D/loss_real'] = d_loss_real.item()\n",
    "                loss['D/loss_fake'] = d_loss_fake.item()\n",
    "                loss['D/loss_cls'] = d_loss_cls.item()\n",
    "                loss['D/loss_gp'] = d_loss_gp.item()\n",
    "            \n",
    "                # =================================================================================== #\n",
    "                #                               3. Train the generator                                #\n",
    "                # =================================================================================== #\n",
    "\n",
    "                if (i+1) % self.n_critic == 0:\n",
    "                    # Original-to-target domain.\n",
    "                    x_fake = self.G(x_real, c_trg)\n",
    "                    out_src, out_cls = self.D(x_fake)\n",
    "                    out_cls = out_cls[:, :self.c_dim] if dataset == 'CelebA' else out_cls[:, self.c_dim:]\n",
    "                    g_loss_fake = - torch.mean(out_src)\n",
    "                    g_loss_cls = self.classification_loss(out_cls, label_trg, dataset)\n",
    "\n",
    "                    # Target-to-original domain.\n",
    "                    x_reconst = self.G(x_fake, c_org)\n",
    "                    g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n",
    "\n",
    "                    # Backward and optimize.\n",
    "                    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls\n",
    "                    self.reset_grad()\n",
    "                    g_loss.backward()\n",
    "                    self.g_optimizer.step()\n",
    "\n",
    "                    # Logging.\n",
    "                    loss['G/loss_fake'] = g_loss_fake.item()\n",
    "                    loss['G/loss_rec'] = g_loss_rec.item()\n",
    "                    loss['G/loss_cls'] = g_loss_cls.item()\n",
    "\n",
    "                # =================================================================================== #\n",
    "                #                                 4. Miscellaneous                                    #\n",
    "                # =================================================================================== #\n",
    "\n",
    "                # Print out training info.\n",
    "                if (i+1) % self.log_step == 0:\n",
    "                    et = time.time() - start_time\n",
    "                    et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "                    log = \"Elapsed [{}], Iteration [{}/{}], Dataset [{}]\".format(et, i+1, self.num_iters, dataset)\n",
    "                    for tag, value in loss.items():\n",
    "                        log += \", {}: {:.4f}\".format(tag, value)\n",
    "                    print(log)\n",
    "\n",
    "                    if self.use_tensorboard:\n",
    "                        for tag, value in loss.items():\n",
    "                            self.logger.scalar_summary(tag, value, i+1)\n",
    "\n",
    "            # Translate fixed images for debugging.\n",
    "            if (i+1) % self.sample_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    x_fake_list = [x_fixed]\n",
    "                    for c_fixed in c_celeba_list:\n",
    "                        c_trg = torch.cat([c_fixed, zero_rafd, mask_celeba], dim=1)\n",
    "                        x_fake_list.append(self.G(x_fixed, c_trg))\n",
    "                    for c_fixed in c_rafd_list:\n",
    "                        c_trg = torch.cat([zero_celeba, c_fixed, mask_rafd], dim=1)\n",
    "                        x_fake_list.append(self.G(x_fixed, c_trg))\n",
    "                    x_concat = torch.cat(x_fake_list, dim=3)\n",
    "                    sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n",
    "                    save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
    "                    print('Saved real and fake images into {}...'.format(sample_path))\n",
    "\n",
    "            # Save model checkpoints.\n",
    "            if (i+1) % self.model_save_step == 0:\n",
    "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
    "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
    "                torch.save(self.G.state_dict(), G_path)\n",
    "                torch.save(self.D.state_dict(), D_path)\n",
    "                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
    "\n",
    "            # Decay learning rates.\n",
    "            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
    "                g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
    "                d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
    "                self.update_lr(g_lr, d_lr)\n",
    "                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Translate images using StarGAN trained on a single dataset.\"\"\"\n",
    "        # Load the trained generator.\n",
    "        self.restore_model(self.test_iters)\n",
    "        \n",
    "        # Set data loader.\n",
    "        if self.dataset == 'CelebA':\n",
    "            data_loader = self.celeba_loader\n",
    "        elif self.dataset == 'RaFD':\n",
    "            data_loader = self.rafd_loader\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_real, c_org) in enumerate(data_loader):\n",
    "\n",
    "                # Prepare input images and target domain labels.\n",
    "                x_real = x_real.to(self.device)\n",
    "                c_trg_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n",
    "\n",
    "                # Translate images.\n",
    "                x_fake_list = [x_real]\n",
    "                for c_trg in c_trg_list:\n",
    "                    x_fake_list.append(self.G(x_real, c_trg))\n",
    "\n",
    "                # Save the translated images.\n",
    "                x_concat = torch.cat(x_fake_list, dim=3)\n",
    "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
    "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
    "                print('Saved real and fake images into {}...'.format(result_path))\n",
    "\n",
    "    def test_multi(self):\n",
    "        \"\"\"Translate images using StarGAN trained on multiple datasets.\"\"\"\n",
    "        # Load the trained generator.\n",
    "        self.restore_model(self.test_iters)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (x_real, c_org) in enumerate(self.celeba_loader):\n",
    "\n",
    "                # Prepare input images and target domain labels.\n",
    "                x_real = x_real.to(self.device)\n",
    "                c_celeba_list = self.create_labels(c_org, self.c_dim, 'CelebA', self.selected_attrs)\n",
    "                c_rafd_list = self.create_labels(c_org, self.c2_dim, 'RaFD')\n",
    "                zero_celeba = torch.zeros(x_real.size(0), self.c_dim).to(self.device)            # Zero vector for CelebA.\n",
    "                zero_rafd = torch.zeros(x_real.size(0), self.c2_dim).to(self.device)             # Zero vector for RaFD.\n",
    "                mask_celeba = self.label2onehot(torch.zeros(x_real.size(0)), 2).to(self.device)  # Mask vector: [1, 0].\n",
    "                mask_rafd = self.label2onehot(torch.ones(x_real.size(0)), 2).to(self.device)     # Mask vector: [0, 1].\n",
    "\n",
    "                # Translate images.\n",
    "                x_fake_list = [x_real]\n",
    "                for c_celeba in c_celeba_list:\n",
    "                    c_trg = torch.cat([c_celeba, zero_rafd, mask_celeba], dim=1)\n",
    "                    x_fake_list.append(self.G(x_real, c_trg))\n",
    "                for c_rafd in c_rafd_list:\n",
    "                    c_trg = torch.cat([zero_celeba, c_rafd, mask_rafd], dim=1)\n",
    "                    x_fake_list.append(self.G(x_real, c_trg))\n",
    "\n",
    "                # Save the translated images.\n",
    "                x_concat = torch.cat(x_fake_list, dim=3)\n",
    "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
    "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
    "                print('Saved real and fake images into {}...'.format(result_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea935cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(beta1=0.5, beta2=0.999, c2_dim=8, c_dim=5, d_conv_dim=64, d_lr=0.0001, d_repeat_num=6, dataset='CelebA', g_conv_dim=64, g_lr=0.0001, g_repeat_num=6, image_size=256, mode='test', model_save_dir='stargan_celeba_256/models', num_workers=1, selected_attrs=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'], test_iters=200000)\n"
     ]
    }
   ],
   "source": [
    "# args와 동일한 형태의 데이터 인자를 넘기기 위한 테크닉\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace()\n",
    "\n",
    "# Training configuration.\n",
    "args.dataset = \"CelebA\"\n",
    "args.selected_attrs = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Male\", \"Young\"]\n",
    "args.g_lr = 0.0001\n",
    "args.d_lr = 0.0001\n",
    "args.beta1 = 0.5\n",
    "args.beta2 = 0.999\n",
    "\n",
    "# Test configuration.\n",
    "args.test_iters = 200000 # 20만번 학습을 돌린 모델\n",
    "\n",
    "# Model configurations.\n",
    "args.c_dim = 5\n",
    "args.c2_dim = 8\n",
    "args.image_size = 256\n",
    "args.g_conv_dim = 64\n",
    "args.d_conv_dim = 64\n",
    "args.g_repeat_num = 6\n",
    "args.d_repeat_num = 6\n",
    "\n",
    "# Directories.\n",
    "args.model_save_dir = \"stargan_celeba_256/models\"\n",
    "\n",
    "# Miscellaneous.\n",
    "args.num_workers = 1\n",
    "args.mode = \"test\"\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13e6a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = Solver(args)\n",
    "# solver.restore_model(solver.test_iters) # 200000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819034ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
